{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of OLS Estimators\n",
    "Now we will look at some properties of the OLS estimator. Recall the properties of estimators from Lesson 3.\n",
    "- Consistency\n",
    "- Unbiasedness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbiasedness of OLS Estimator\n",
    "Let us suppose we have an iid sample $(Y_i,X_i)_{i=1}^N$, where $X_i$ is a $(K+1)\\times 1$ random vector. Let the regression of $Y$ on $X$ be parameterized by a $K+1\\times 1$ vector $\\beta$, i.e., $BLP(Y|X) = X'\\beta$. The OLS estimator of $\\beta$ is\n",
    "$$\n",
    "\\hat{\\beta}_N = \\left(\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\sum_{i=1}^NX_iY_i = (\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}\\boldsymbol{Y},\n",
    "$$\n",
    "where again, $\\boldsymbol{X}$ denotes a $N\\times (K+1)$ matrix of data, whose $ith$ row is $X_i'$, and $\\boldsymbol{Y}$ is a $N\\times 1$ vector with $ith$ element equal to $Y_i$.\n",
    "\n",
    "In order for $\\hat{\\beta}_N$ to be an unbiased estimator of $\\beta$, we also require that the regression residual, defined as $U = Y - X'\\beta$, be mean independent of $X$, i.e., $E[U|X] = E[U] = 0$. The proof of unbiasedness proceeds as follows:\n",
    "$$\n",
    "\\hat{\\beta}_N = \\left(\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\sum_{i=1}^NX_iY_i\n",
    "$$\n",
    "$$\n",
    "= \\left(\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\sum_{i=1}^NX_i(X_i'\\beta + (Y_i - X_i'\\beta))\n",
    "$$\n",
    "$$\n",
    "= \\beta + \\left(\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\sum_{i=1}^NX_i'(Y_i - X_i'\\beta)\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "E[\\hat{\\beta}_N] = \\beta + E\\left[\\left(\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\sum_{i=1}^NX_i'(Y_i - X_i'\\beta)\\right]\n",
    "$$\n",
    "$$\n",
    "= \\beta + E\\left[\\left(\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\sum_{i=1}^NX_i'U_i\\right]\n",
    "$$\n",
    "and by the Law of Iterated Expectations,\n",
    "$$\n",
    "E\\left[\\left(\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\sum_{i=1}^NX_i'U_i\\right] = E\\left[E\\left[\\left(\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\sum_{i=1}^NX_i'U_i\\bigg|X_1,X_2,...,X_N\\right]\\right]\n",
    "$$\n",
    "$$\n",
    "= E\\left[\\left(\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\sum_{i=1}^NX_i'E\\left[U_i\\bigg|X_1,X_2,...,X_N\\right]\\right] = 0\n",
    "$$\n",
    "since $E[U_i|X_1,X_2,...,X_N] = E[U_i|X_i] = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency of OLS Estimator\n",
    "Following the first couple of steps for the proof of unbiasedness above, rewrite $\\hat{\\beta}_N$ as\n",
    "$$\n",
    "\\hat{\\beta}_N = \\beta + \\left(\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\sum_{i=1}^NX_i'(Y_i - X_i'\\beta).\n",
    "$$\n",
    "What do we know about the components on the RHS? We could rewrite them by putting $1/N$ in the inverse component and the other component, obtaining\n",
    "$$\n",
    "\\left(\\frac{1}{N}\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\frac{1}{N}\\sum_{i=1}^NX_i'(Y_i - X_i'\\beta).\n",
    "$$\n",
    "The component in parantheses is just a matrix of sample means. You can apply the WLLN to each of its components so that\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^NX_iX_i' \\overset{p}{\\to} E[XX'].\n",
    "$$\n",
    "The WLLN can also be applied to the other component, so that\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^NX_i'(Y_i - X_i'\\beta) \\overset{p}{\\to} E[X'(Y - X'\\beta)] = 0,\n",
    "$$\n",
    "where the last equality holds from properties of BLP (regression). Therefore, by applying the CMT, \n",
    "$$\n",
    "\\hat{\\beta}_N \\overset{p}{\\to} \\beta + (E[XX'])^{-1}E[X'(Y-X'\\beta)] = \\beta + (E[XX'])^{-1}\\cdot 0 = \\beta.\n",
    "$$\n",
    "So that we've proved consistency of the OLS estimator of the regression parameters $\\beta$. We DID NOT need to assume mean independence, $E[U|X]=E[U]=0$, as we did to prove unbiasedness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asymptotic Distribution of OLS Estimator\n",
    "If you guessed that the OLS estimator would be asymptotically normal, you'd be correct. How do we prove this? Consider the random vector $X_i(Y_i - X_i\\beta)=X_iU_i$, where we define $U_i=Y_i - X_i\\beta$. If our sample is $(X_i,Y_i)_{i=1}^N$ and each observation is iid, then by the Central Limit Theorem, we have that\n",
    "$$\n",
    "\\sqrt{N}\\frac{1}{N}\\sum_{i=1}^NX_iU_i \\overset{d}{\\to} \\mathcal{N}(\\boldsymbol{0}, Var[XU])\n",
    "$$\n",
    "where $boldsymbol{0}$ is a $(K+1)\\times 1$ vector of zeros, and $Var[XU]$ is a $(K+1)\\times(K+1)$ variance matrix. Notice that since $E[XU]=0$ then $Var[XU]=E[XU^2X']$.\n",
    "\n",
    "We will use this convergence result in a bit. For now, rewrite our estimator $\\hat{\\beta}_N$ as in previous proofs of consistency and unbiasedness, as\n",
    "$$\n",
    "\\hat{\\beta}_N = \\beta + \\left(\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\sum_{i=1}^NX_i'U_i\n",
    "$$\n",
    "$$\n",
    "= \\beta + \\left(\\frac{1}{N}\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\frac{1}{N}\\sum_{i=1}^NX_i'U_i,\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "\\sqrt{N}(\\hat{\\beta}_N - \\beta) = \\left(\\frac{1}{N}\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\sqrt{N}\\frac{1}{N}\\sum_{i=1}^NX_i'U_i\n",
    "$$\n",
    "Now we can apply the WLLN to the component in parantheses, as above, and use our convergence result from above for the component on the right, then by apply the Slutzky Theorem, we obtain\n",
    "$$\n",
    "\\sqrt{N}(\\hat{\\beta}_N - \\beta) \\overset{d}{\\to} \\mathcal{N}(\\boldsymbol{0}, E[XX']^{-1}E[XU^2X']E[XX']^{-1}).\n",
    "$$\n",
    "Denote the variance component as $V=E[XX']^{-1}E[XU^2X']E[XX']^{-1}$.\n",
    "\n",
    "How do we estimate $V$? Let $\\hat{V}_N$ denote the estimator for $V$ obtained by replacing population moments with sample moments. \n",
    "$$\n",
    "\\hat{V}_N  = \\left(\\frac{1}{N}\\boldsymbol{X}'\\boldsymbol{X}\\right)^{-1}\\frac{1}{N}\\left(\\boldsymbol{X}'\\hat{\\Omega}\\boldsymbol{X}\\right)\\left(\\frac{1}{N}\\boldsymbol{X}'\\boldsymbol{X}\\right)^{-1} \n",
    "$$\n",
    "$$ = N\\left(\\boldsymbol{X}'\\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}'\\hat{\\Omega}\\boldsymbol{X}\\right)\\left(\\boldsymbol{X}'\\boldsymbol{X}\\right)^{-1}\n",
    "$$\n",
    "where $\\hat{\\Omega}_N$ is an $N\\times N$ diagonal matrix\n",
    "$$\n",
    "\\hat{\\Omega} =\n",
    "\\begin{pmatrix}\n",
    "\\hat{U}_1^2 & 0 & \\cdots & \\cdots & 0 \\\\\n",
    "0 & \\hat{U}_2^2 & 0 & \\cdots & 0 \\\\\n",
    "\\vdots & 0 & \\ddots & \\vdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & \\cdots & \\cdots & 0 & \\hat{U}_N^2 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The 0s in the off-diagonal entries indicate that the data are independent across observations. An estimator for the variance of $\\hat{\\beta}_N$ is $\\frac{1}{N}\\hat{V}_N$.\n",
    "\n",
    "## Asymptotics when sample is not independently sampled?\n",
    "- As an aside, this class will be primarily concerned with the iid case\n",
    "- However, there are many situations when data are likely not iid\n",
    "- In this case, other Central Limit Theorems and WLLNs which allow for well behaved types of dependence between data can be used to derive similar asymptotics\n",
    "- One example might be data on students in a school. You could assume that there is dependence between students within the same classroom, but that students from different classrooms are independent. This is a good example of clustered data and most software packages have an option to account for this dependence when estimating standard errors and conducting hypothesis tests. On the computer, the dependence will be reflected in how we specifiy the $\\hat{\\Omega}$ matrix. If we cluster on the classroom level, then $\\hat{\\Omega}_N$ will have non-zero entries between students in the same classroom, and 0 entries for students in different classrooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing with OLS Estimator\n",
    "Now that we know how to obtain the asymptotic variance of $\\sqrt{N}(\\hat{\\beta}_N - \\beta)$, it is time to apply this knowledge toward developing a test statistic and conductin hypothesis tests. Let us suppose we have a sample of data $(X_i,Y_i)_{i=1}^N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests for One Parameter\n",
    "Suppose we want to test a hypothesis about the slope parameter $\\beta_1$. Let $H_0:\\beta_1=c$ for some real number $c$. We know from above that, under the null hypothesis, $\\sqrt{N}(\\hat{\\beta}_{1,N} - c)\\overset{d}{\\to}\\mathcal{N}(0,V_{22})$, where $V_{22}$ denotes the element in the 2nd row and 2nd column of $V$. Therefore, denoting $\\hat{V}_{22,N}$ as the sample counterpart of $V_{22}$, we have that $\\tau_N = \\sqrt{N}\\frac{\\hat{\\beta}_{1,N} - c}{\\sqrt{\\hat{V}_{22,N}}}\\overset{d}{\\to}\\mathcal{N}(0,1)$. \n",
    "\n",
    "- Aside: Note that $\\tau_N$ is often referred to as the \"t-stat\" or \"t-statistic,\" the reason being than under normality of errors its exact distribution is a student's t distribution. \n",
    "\n",
    "Our test will be\n",
    "$$\n",
    "\\phi_N = \\boldsymbol{1}\\left\\{\\bigg|\\tau_N\\bigg|\\geq z_{1-\\alpha/2}\\right\\}\n",
    "$$\n",
    "and therefore our p-value is $\\hat{p}_N = 2(1 - \\Phi(|\\tau_N|))$. \n",
    "\n",
    "Next, we will simulate some data and perform this test for the hypothesis that $c=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.0193751991989326"
      ],
      "text/latex": [
       "0.0193751991989326"
      ],
      "text/markdown": [
       "0.0193751991989326"
      ],
      "text/plain": [
       "[1] 0.0193752"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sim_data = function(N, b0=2, b1=1, x.var=1, x.mean=3, noise.var=20) {\n",
    "    x = rnorm(N, mean=x.mean, sd=sqrt(x.var))\n",
    "    noise = rnorm(N, mean=0, sd=sqrt(noise.var))\n",
    "    y = b0 + b1*x + noise\n",
    "    \n",
    "    return(cbind(y, rep(1, N), x))\n",
    "}\n",
    "\n",
    "N = 50\n",
    "set.seed(210)\n",
    "\n",
    "data = sim_data(N)\n",
    "Y = data[, 1]\n",
    "X = data[, 2:3]\n",
    "\n",
    "# Defining function to estimate regression parameters\n",
    "reg = function(Y, X){solve(t(X)%*%X)%*%t(X)%*%Y}\n",
    "\n",
    "# Defining function to return regression residuals\n",
    "res = function(Y, X){Y - X%*%reg(Y, X)}\n",
    "\n",
    "b.hat = reg(Y, X)\n",
    "U.hat = res(Y, X)\n",
    "\n",
    "# Multiplying row in X by corresponding element in U.hat\n",
    "XU = sweep(X, MARGIN=1, U.hat, `*`)\n",
    "\n",
    "# Calculating variance matrix\n",
    "V = N*solve(t(X)%*%X)%*%t(XU)%*%XU%*%solve(t(X)%*%X)\n",
    "\n",
    "# Test statistic\n",
    "c = 0\n",
    "t.stat = sqrt(N)*(b.hat[2] - c)/sqrt(V[2, 2])\n",
    "p.val = 2*(1 - pnorm(t.stat))\n",
    "p.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Joint) Tests of Multiple Parameters\n",
    "Suppose now that our hypothesis is $H_0:R\\beta = Rc$ for some $q\\times (K+1)$ matrix $R$, where $q\\leq K+1$, and $(K+1)\\times 1$ vector $c$ (recall that $\\beta$ is a $K+1$ length vector). \n",
    "\n",
    "- Note: When $R$ is the identity matrix and $c=(0,0,...,0)'$ then our null hypothesis is $H_0:\\beta_0=\\beta_1=...\\beta_K=0$.\n",
    "\n",
    "We know that under the null hypothesis of $H_0:\\beta=c$, that $\\sqrt{N}(\\hat{\\beta}_N - c)\\overset{d}{\\to}\\mathcal{N}(0,V)$. But what about the asymptotic distribution of $\\sqrt{N}(R\\hat{\\beta}_N - Rc)$? It turns out, by the Delta method (no need to know this), that under the null hypothesis that $H_0:R\\beta = Rc$, that \n",
    "$$\n",
    "\\sqrt{N}(R\\hat{\\beta}_N - Rc)\\overset{d}{\\to}\\mathcal{N}(0,RVR').\n",
    "$$\n",
    "\n",
    "Now consider the following:\n",
    "$$\n",
    "N(\\hat{R\\beta}_N - Rc)'(R\\hat{V}_NR')^{-1}(R\\hat{\\beta}_N - Rc).\n",
    "$$\n",
    "This is scalar because it is a $1\\times q$ multiplied by a $q\\times q$ multiplied by a $q\\times 1$ matrix. Just a quick glance reveals that we are summing $q$ squares of random variables whose asymptotic distribution is a $\\mathcal{N}(0,1)$ random variable. Thus, the asymptotic distribution is a sum of $q$ squared standard normal random variables. This is the definition of a chi-squared distribution with $q$ degrees of freedom. It turns out that\n",
    "$$\n",
    "\\tau_N = N(\\hat{R\\beta}_N - Rc)'(R\\hat{V}_NR')^{-1}(R\\hat{\\beta}_N - Rc)\\overset{d}{\\to}\\chi^2_q,\n",
    "$$\n",
    "that is, a chi-squared distribution with $q$ degrees of freedom. Let $c_{q,1 - \\alpha}$ denote the $1-\\alpha$ percentile of a chi-squared distribution with $q$ degrees of freedom.\n",
    "\n",
    "- Aside: Note that we can define $F_N = \\tau_N/q$ to be what is commonly known as the F-statistic. This statistic has an asymptotic distribution of $F_{q,\\infty}$, i.e., an F-distribution with $q$ degrees of freedom in the numerator and $\\infty$ degrees of freedom in the denominator. We mention this statistic because it is commonly reported in regression tables.\n",
    "\n",
    "Then our test is $\\phi_N = \\boldsymbol{1}\\{\\tau_N > c_{q,1-\\alpha}\\}$. Our p-value is then defined by $\\hat{p}_N = 1 - F_{\\chi,q}(\\tau_N)$ where $F_{\\chi,q}$ is the cdf of a chi-squared distribution with $q$ degrees of freedom.\n",
    "\n",
    "Let's conduct this test using $c=(2,0.5)'$ and $R$ as the identity matrix, using the data we created for our Monte Carlo exercise above. In this example, $K=1$ and $q=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>0.07190508</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       "\t 0.07190508\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 0.07190508 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]      \n",
       "[1,] 0.07190508"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = rbind(c(2), c(0.5))\n",
    "R = diag(2)\n",
    "q = 2\n",
    "\n",
    "t.stat = N*t(R%*%b.hat - R%*%c)%*%solve(R%*%V%*%t(R))%*%(R%*%b.hat - R%*%c)\n",
    "p.val = 1 - pchisq(t.stat, df=q)\n",
    "p.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that this is equivalent to performing the test using the cdf of an F-distribution with $q$ and $\\infty$ degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>0.07190508</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       "\t 0.07190508\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 0.07190508 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]      \n",
       "[1,] 0.07190508"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "1 - pf(t.stat/q, df1=q, df2=Inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency of OLS: The Gauss-Markov Theorem\n",
    "We've already seen that the OLS estimator is consistent for the linear regression parameter $\\beta$ and that it is unbiased when we additionally assume $E[U_i|X_1,...,X_N] = 0$, where again, $U_i$ is defined as $Y_i - X_i'\\beta$. We've also worked out the asymptotic variance of the estimator, which we then used for hypothesis testing.\n",
    "\n",
    "If we assume $E[U_i|X_1,...,X_N]=0$ and $Var[U_i|X_1,...,X_N]=0$ then the OLS estimator is the \"best linear unbiased estimator\" of $\\beta$.  By best, we mean that it the estimator is of minimal variance in some sense."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
