{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toward a Further Understanding of the Relationship Between Variables\n",
    "We've reviewed all the tools in statistics we need to push further into new territory. But before we start to apply this knowledge toward learning about the economic forces generating the data, we need to further develop our understanding of how statistics describes the relationship between variables. Economics describes the structural dependence between variables whereas statistics describes distributional dependence. Econometrics as about how to utilize statistics or distributional dependence to understand structural dependence. But more on this later. For now, let's retreat to the more familiar domain of statistics and distributional dependence.\n",
    "\n",
    "We've seen a few concepts that measure this dependence, including conditional means and covariance. We will elaborate further on these concepts as we proceed to regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "There is plenty of confusion about what regression actually means. It is sometimes referred to as any study of the conditional distribution of a variable $Y$ and a single or vector of (independent) variables $X$. This could include how the quantiles of $Y$ change with $X$. But for this class, regression analysis will be the study of the conditional mean $E[Y|X=x]$, that is, how the mean or expectation of $Y$ changes for different realizations of the random variable $X$.\n",
    "\n",
    "We've already discussed that $E[Y|X=x]$ is just a function of $x$. We can denote this function $f(x)$ and call it the regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Linear regression can equivalently be thought of as \"the best linear predictor of Y given $X$,\" or the best linear approximation to $E[Y|X=x]$. In what sense is it best? We can define this best approximation as $g(x) = \\beta_0 + \\beta_1x$ where $\\beta_0$ and $\\beta_1$ are\n",
    "$$\n",
    "(\\beta_0,\\beta_1)  = \\text{arg}\\min_{b_0,b_1}E_X\\big[\\big(E[Y|X] - (b_0 + b_1X)\\big)^2\\big]\n",
    "$$\n",
    "$$\n",
    "= \\text{arg}\\min_{b_0,b_1}E\\big[\\big(Y - (b_0 + b_1X)\\big)^2\\big]\n",
    "$$\n",
    "where the second equality follows by working out the first equality. So what exactly are $\\beta_0$ and $\\beta_1$ in terms of concepts we already know? Let's take FOCs of the second optimization problem. The FOC with respect to $b_1$ and evaluated at $(b_0,b_1)=(\\beta_0,\\beta_1)$ is\n",
    "$$\n",
    "2E\\big[X\\big(Y - (\\beta_0 + \\beta_1X)\\big)\\big] = 0\n",
    "$$\n",
    "$$\n",
    "\\implies  E[XY - \\beta_0X - \\beta_1X^2] = 0\n",
    "$$\n",
    "$$\n",
    "\\implies  \\beta_1 = \\frac{E[XY] - \\beta_0E[X]}{E[X^2]}\n",
    "$$\n",
    "and the FOC for $b_0$ and evaluated at $(b_0,b_1)=(\\beta_0,\\beta_1)$ is\n",
    "$$\n",
    "E[Y - \\beta_0 - \\beta_1X] = 0\n",
    "$$\n",
    "$$\n",
    "\\implies  \\beta_0 = E[Y] - \\beta_1E[X]\n",
    "$$\n",
    "and inserting this expression for $\\beta_0$ in the expression for $\\beta_1$ we obtain\n",
    "$$\n",
    "\\beta_1  = \\frac{E[XY] - \\left(E[Y] - \\beta_1E[X]\\right)E[X]}{E[X^2]}\n",
    "$$\n",
    "$$\n",
    "= \\frac{E[XY] - E[Y]E[X] + \\beta_1E[X]^2}{E[X^2]}\n",
    "$$\n",
    "$$\n",
    "\\implies \\beta_1  = \\frac{E[XY] - E[Y]E[X]}{E[X^2] - E[X]^2} = \\frac{Cov[X,Y]}{Var[X]},\n",
    "$$\n",
    "which is a nice expression which shows that the linear relationship between the mean of $Y$ and $X$ has a slope proportional to the covariance between the two. Thus, the covariance has a direct link to understanding the linear relationship between $Y$ and $X$. We will denote $\\beta_0 + \\beta_1X$ as $BLP(Y|X)$ and refer to this as the best linear predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of the BLP\n",
    "1. From the FOC, $E[X(Y-BLP(Y|X))] = 0$. We may sometimes define the regression residual $U=Y-BLP(Y|X)$. Thus, this condition is equivalent to $E[XU]=0$.\n",
    "2. $BLP(Y|X)=\\beta_0 + \\beta_1X$ is a linear function of $X$, and is therefore a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression using Matrix Notation\n",
    "As brief aside, we'll sometimes find it useful to write everything in matrix notation. Actually, because of the ubiquity of computer use in the profession nowadays, its essential that we introduce matrix notation sooner, because the way we will work on our computers is to use vectors and matrices to represent our data.\n",
    "\n",
    "Suppose we grouped everything, denoting the random variable $X$ with $X_1$ and redefining $X$ as the random vector $X=(1,X_1)'$ so that $X$ is now a $2\\times 1$ (2 rows, 1 column) vector and define $\\beta = (\\beta_0, \\beta_1)'$ so $\\beta$ is also a $2\\times 1$ vector. In this class, we will always think of a vector as a column vector, meaning a $K\\times 1$ matrix. This should help you remember dimensions.\n",
    "\n",
    "By writing $X'$ we mean the transpose of $X$. The transpose of $X$ is a $1\\times 2$ vector. Define $b=(b_0, b_1)'$ and $\\hat{\\beta}_N$ similarly. Then we can write $\\beta_0 + \\beta_1X_1$ in matrix notation as $X'\\beta$ in our new notation. Then our maximization problem from above would look like\n",
    "$$\n",
    "\\max_bE[(Y - X'b)^2]\n",
    "$$\n",
    "and our FOCs could be very simply written as\n",
    "$$\n",
    "0 = E[X(Y - X'b)] =\n",
    "\\begin{pmatrix}\n",
    "E[1\\cdot(Y - X'b)] \\\\ E[X_1(Y - X'b)]\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "E[1\\cdot(Y - b_0 - b_1X_1)] \\\\ E[X_1(Y - b_0 - b_1X_1)].\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "What is nice about matrix notation is that we can write the solution with much greater ease. Taking the FOCs above and using $\\beta$ in place of $b$, we have\n",
    "$$\n",
    "0  = E[X(Y - X'\\beta)] = E[XY] - E[XX']\\beta \\implies \\beta = E[XX']^{-1}E[XY]\n",
    "$$\n",
    "$$ = \\begin{pmatrix}\n",
    "E[1^2] & E[1\\cdot X_1] \\\\\n",
    "E[X_1\\cdot 1] & E[X_1^2]\n",
    "\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}\n",
    "E[1\\cdot Y] \\\\ E[X_1Y]\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "which can readily be expressed to look like the solution we obtained above for $\\beta_0$ and $\\beta_1$.\n",
    "- Note that we could have just as readily defined $X$ as an arbitrary $K\\times 1$ random vector $X=(X_1,X_2,...,X_K)'$ and the above matrix notation would be exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditions Required for Regression\n",
    "1. Invertibility of $E[XX']$. $E[XX']$ is a $2\\times 2$ matrix and we need to be able to invert it! $E[XX']$ is invertible if and only if $X$ has no perfect collinearity, i.e., $\\nexists$ a non-zero  vector $c\\in\\boldsymbol{R}^2$ such that $P(X'c = 0)=1$. This is just a mathy way of saying that the components of $X$ can't be linear combinations of one another for \"everyone.\" In our example with $X=(1, X_1)$, this means that there is some variation in $X_1$, i.e., $Var[X_1]>0$. You can get this result by using the fact that the determinant of an invertible matrix is non-zero, and $det(E[XX'])=Var[X_1]$.\n",
    "\n",
    "### Looking closer at invertibility\n",
    "Why is no perfect collinearity equivalent to invertibility of $E[XX']$? Recall that a matrix is invertible if and only if it is positive definite and symmetric. $E[XX']$ is clearly symmetric. So $E[XX']$ is invertible if and only if it is positive definite. A matrix $M$ is positive definite if for all $c\\neq 0$, $c'Mc>0$. In the case of $E[XX']$ this means that $c'E[XX']c>0$ for all $c\\neq 0$. Let's see if we can show the equivalence between this case and our definition of no perfect collinearity.\n",
    "\n",
    "**Proof.** $\\rightarrow$. Suppose we have no perfect collinearity in $X$. Then there does not exist $c\\neq 0$ such that $P(X'c=0)=1$. Therefore, for all $c\\neq 0$, $P(X'c=0)<1$. Therefore, $P(X'c\\neq 0)>0$. Then, using that $c'E[XX']c=E[c'XX'c]$, it must be that $E[c'XX'c]>0$ since $c'XX'c\\geq 0$ (this is just a sum of squares) and $P(X'c\\neq 0)>0$. $\\leftarrow$ Now suppose $E[XX']$ is invertible. Then it is positive definite and symmetric. Thus, for all $c\\neq 0$, $c'E[XX']c>0$. Furthermore, $E[c'XX'c]>0$. But since $c'XX'c\\geq 0$, in order for the strict inequality to hold, we must have $P(X'c\\neq 0) > 0$. Therefore, $P(X'c=0)<1$ for $c\\neq 0$. Or, $\\nexists c$ such that $P(X'c=0)=1$.\n",
    "\n",
    "### Equivalent conditions for invertibility\n",
    "Suppose $W$ is a square, $K\\times K$ matrix. The following conditions are equivalent to $W$ being invertible.\n",
    "1. If we further assume that $W$ is symmetric, then $W$ is invertible if and only if it is positive definite, where positive definite means for all $c\\neq 0$, $c'Wc>0$.\n",
    "2. $W$ has non-vanishing determinant, i.e., $det(W)\\neq 0$\n",
    "3. All eigenvalues of $W$ are positive, where an eigenvalue $\\lambda$ is such that $Wc=\\lambda c$ for some $c\\neq 0$. \n",
    "4. $rank(W)=row(W)=col(W)=K$, where $row(\\cdot)$ and $col(\\cdot)$ denote the dimension of the row and column spaces (i.e., the number of linearly independent rows and columns), respectively. These are sometimes called the row or column ranks of the matrix. This condition is sometimes called \"the full rank condition.\"\n",
    "\n",
    "To understand invertibility of matrices, it is important to first remember the definition of invertibility for a function and then to notice that matrices can be considered linear mappings (i.e., linear functions). The intuition of invertibility of a matrix follows from recalling that a linear function is invertible if it is a bijective linear mapping. When a matrix is square and has full rank, then it is a bijective linear mapping, and therefore has an inverse.\n",
    "\n",
    "Why is a matrix is linear mapping? Consider a $N\\times K$ matrix $W$. You can take any vector $c$ of length $N$ and right multiply it by $W$ to obtain a vector of length $K$, e.g, $c'W = v$ for some $N\\times 1$ vector $c$ and $K\\times 1$ vector $v$. Thus, $W$ just represents a function $f:\\boldsymbol{R}^N\\to\\boldsymbol{R}^K$. In order for $f$ to be bijective, it needs to be one-to-one and onto, which translates into the matrix $W$ representing $f$ to be square and have columns/rows that are linearly independent.\n",
    "\n",
    "### Quick example of non-invertibility\n",
    "Let $X=(1,X_1,X_2)'$ where $X_1$ takes on a value of 1 if male and 0 if female and $X_2$ takes a value of 1 if female and 0 if male. Thus, you can interpret $X_1$ as a binary indicator for male and $X_2$ as a binary indicator for female. Let $c=(-1, 1, 1)'$. Then $X'c=-1 + X_1 + X_2$ but since $X_1=1-X_2$, then $X'c = -1 + (1-X_2) + X_2 = 0$. Thus, we have shown $\\exists$ c such that $P(X'c=0)=1$, and there is perfect collinearity in $X$. Therefore $E[XX']$ is not invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators of the Linear Regression Function\n",
    "Now how would we go about finding estimators for $\\beta_0$ and $\\beta_1$? Going back to our lesson on constructing estimators, we learned three approaches: replacing population moments with their sample counterparts, least squares and maximum likelihood. We noticed that all three approaches happened to achieve the same results. We will obtain similar results when estimating the linear regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sample Moments as Estimates of Population Moments\n",
    "Then we obtain estimators\n",
    "$$\n",
    "\\hat{\\beta}_{1,N}  = \\frac{\\frac{1}{N}\\sum_{i=1}^N(Y_i - \\bar{Y}_N)(X_i - \\bar{X}_N)}{\\frac{1}{N}\\sum_{i=1}^N(X_i - \\bar{X}_N)^2} = \\frac{\\hat{\\sigma}_{XY}}{\\hat{\\sigma}_X^2}\n",
    "$$\n",
    "$$\n",
    "\\hat{\\beta}_{0,N}  = \\bar{Y}_N - \\hat{\\beta}_{1,N}\\bar{X}_N.\n",
    "$$\n",
    "We next show that using least squares we obtain the same result as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Ordinary Least Squares (OLS) Estimator\n",
    "Suppose we have a sample of data $(Y_i,X_i)_{i=1}^N$. Similar to our previous experience with least squares, let's solve the following problem:\n",
    "$$\n",
    "\\min_{b_0,b_1}\\sum_{i=1}^N\\big(Y_i - (b_0 + b_1X_i)\\big)^2.\n",
    "$$\n",
    "Taking FOCs w.r.t $b_0$ and $b_1$ and letting $\\hat{\\beta}_{0,N}$ and $\\hat{\\beta}_{1,N}$ denote our estimators, we have\n",
    "$$\n",
    "2\\sum_{i=1}^NX_i(Y_i - \\hat{\\beta}_{0,N} - \\hat{\\beta}_{1,N}X_i) = 0\n",
    "$$\n",
    "$$\n",
    "\\sum_{i=1}^N(Y_i - \\hat{\\beta}_{0,N} - \\hat{\\beta}_{1,N}X_i) = 0,\n",
    "$$\n",
    "which, solving for $\\hat{\\beta}_{0,N}$ and $\\hat{\\beta}_{1,N}$ we obtain\n",
    "$$\n",
    "\\hat{\\beta}_{1,N} = \\frac{\\frac{1}{N}\\sum_{i=1}^N(Y_i - \\bar{Y}_N)(X_i - \\bar{X}_N)}{\\frac{1}{N}\\sum_{i=1}^N(X_i - \\bar{X}_N)^2} = \\frac{\\hat{\\sigma}_{XY}}{\\hat{\\sigma}_X^2}\n",
    "$$\n",
    "$$\n",
    "\\hat{\\beta}_{0,N} = \\bar{Y}_N - \\hat{\\beta}_{1,N}\\bar{X}_N.\n",
    "$$\n",
    "which is exactly what we got by replacing sample moments for their population counterparts in the expressions for $\\beta_0$ and $\\beta_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The OLS Estimator using Matrix Notation\n",
    "\n",
    "Just as we did above to define $\\beta$, we can now define $\\hat{\\beta}_N$ using matrix algebra. Denote $X_i=(1, X_{1,i})'$ as a $2\\times 1$ vector. Our problem is to solve\n",
    "$$\n",
    "\\min_b\\sum_{i=1}^N(Y_i - X_i'b)^2\n",
    "$$\n",
    "Taking FOCs and taking $\\hat{\\beta}_N$ as our maximizer, we obtain\n",
    "$$\n",
    "0  = \\sum_{i=1}^NX_i(Y_i - X_i'\\hat{\\beta}_N) = \\sum_{i=1}^NX_iY_i - \\sum_{i=1}^NX_iX_i'\\hat{\\beta}_N\n",
    "$$\n",
    "$$\n",
    "\\implies \\hat{\\beta}_N  = \\left(\\sum_{i=1}^NX_iX_i'\\right)^{-1}\\sum_{i=1}^NX_iY_i\n",
    "$$\n",
    "$$ = \\begin{pmatrix}\n",
    "\\sum_{i=1}^N1^2 & \\sum_{i=1}^NX_{1,i} \\\\\n",
    "\\sum_{i=1}^NX_{1,i} & \\sum_{i=1}X_{1,i}^2\n",
    "\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}\n",
    "\\sum_{i=1}^NY_i \\\\ \\sum_{i=1}^NX_{1,i}Y_i.\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "We can show that we obtain the same answer as above for $\\hat{\\beta}_{0,N}$ and $\\hat{\\beta}_{1,N}$\n",
    "\n",
    "#### Sample Conditions for Regression Estimation\n",
    "1. $\\sum_{i=1}^NX_iX_i'$ must be invertible! In order to be invertible, we must have no perfect collinearity in $X_i$. In practice, when we use actual realizations of $X_i$ (i.e., data) then we just need that there are at least two unique values of $X_{1,i}$ in the data (otherwise $X_{1,i}$ would be perfectly collinear with a constant!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The OLS Estimator Using (Super) Matrix Notation\n",
    "A well versed student of linear algebra would look at what we did above and think we could go one step further. Indeed, we can. Now let $\\boldsymbol{X}$ denote an $N\\times K$ matrix where each row $i$ is $X_i'=(1, X_{1,i})$. Let $\\boldsymbol{Y}$ denote an $N\\times 1$ vector.\n",
    "\n",
    "We talked previously how matrix notation can be useful because we will write code that utilizes matrices and vectors. This is especially true with our (super) matrix notation. Below you'll see that writing computer code that estimates linear regression parameters using OLS will be surprisingly simple! An important note: in this class, we will always think of matrices as having the number of observations $N$ as the number of rows.\n",
    "\n",
    "Our least squares optimization problem now looks like\n",
    "$$\n",
    "\\min_b (\\boldsymbol{Y}-\\boldsymbol{X}b)'(\\boldsymbol{Y}-\\boldsymbol{X}b)\n",
    "$$\n",
    "Taking FOCs we obtain\n",
    "$$\n",
    "0 = \\boldsymbol{X}'(\\boldsymbol{Y} - \\boldsymbol{X}\\hat{\\beta}_N)\n",
    "$$\n",
    "$$\n",
    "\\implies \\hat{\\beta}_N = (\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{Y}\n",
    "$$\n",
    "$$\n",
    "= \\left(\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "X_{1,1} & X_{1,2} & \\cdots & X_{1,N}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1 & X_{1,1} \\\\\n",
    "1 & X_{1,2} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & X_{1, N}\n",
    "\\end{pmatrix}\n",
    "\\right)^{-1}\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "X_{1,1} & X_{1,2} & \\cdots & X_{1,N}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_N\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$ = \\begin{pmatrix}\n",
    "1 + 1 + \\cdots 1 & 1\\cdot X_{1,1} + 1\\cdot X_{1,2} + \\cdots 1\\cdot X_{1,N} \\\\\n",
    "1\\cdot X_{1,1} + 1\\cdot X_{1,2} + \\cdots 1\\cdot X_{1,N} & X_{1,1}^2 + X_{1,2}^2 + \\cdots X_{1,N}^2\n",
    "\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}\n",
    "1\\cdot Y_1 + 1\\cdot Y_2 + \\cdots + 1\\cdot Y_N \\\\\n",
    "X_{1,1}Y_1 + X_{1,2}Y_2 + \\cdots + X_{1,N}Y_N\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$ = \\begin{pmatrix}\n",
    "\\sum_{i=1}^N1^2 & \\sum_{i=1}^NX_{1,i} \\\\\n",
    "\\sum_{i=1}^NX_{1,i} & \\sum_{i=1}X_{1,i}^2\n",
    "\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}\n",
    "\\sum_{i=1}^NY_i \\\\ \\sum_{i=1}^NX_{1,i}Y_i.\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Sample Conditions for Regression Estimation\n",
    "1. $\\boldsymbol{X}'\\boldsymbol{X} = \\sum_{i=1}^NX_iX_i'$ needs to be invertible, as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Your First Regression\n",
    "We will simulate data from a joint distribution of $X$ and $Y$ such that $\\beta_0=2$ and $\\beta_1=10$. Then we will see if our estimates of these regression parameters is close to the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>y</th><th scope=col></th><th scope=col>x</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>28.10027</td><td>1       </td><td>3.030109</td></tr>\n",
       "\t<tr><td>44.11410</td><td>1       </td><td>4.243059</td></tr>\n",
       "\t<tr><td>19.10097</td><td>1       </td><td>2.470715</td></tr>\n",
       "\t<tr><td>27.46297</td><td>1       </td><td>2.409526</td></tr>\n",
       "\t<tr><td>31.40854</td><td>1       </td><td>2.908111</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lll}\n",
       " y &  & x\\\\\n",
       "\\hline\n",
       "\t 28.10027 & 1        & 3.030109\\\\\n",
       "\t 44.11410 & 1        & 4.243059\\\\\n",
       "\t 19.10097 & 1        & 2.470715\\\\\n",
       "\t 27.46297 & 1        & 2.409526\\\\\n",
       "\t 31.40854 & 1        & 2.908111\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "y |  | x | \n",
       "|---|---|---|---|---|\n",
       "| 28.10027 | 1        | 3.030109 | \n",
       "| 44.11410 | 1        | 4.243059 | \n",
       "| 19.10097 | 1        | 2.470715 | \n",
       "| 27.46297 | 1        | 2.409526 | \n",
       "| 31.40854 | 1        | 2.908111 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     y          x       \n",
       "[1,] 28.10027 1 3.030109\n",
       "[2,] 44.11410 1 4.243059\n",
       "[3,] 19.10097 1 2.470715\n",
       "[4,] 27.46297 1 2.409526\n",
       "[5,] 31.40854 1 2.908111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sim_data = function(N, b0=2, b1=10, x.var=1, x.mean=3, noise.var=20){\n",
    "    \n",
    "    x = rnorm(N, mean=x.mean, sd=sqrt(x.var))\n",
    "    noise = rnorm(N, mean=0, sd=sqrt(noise.var))\n",
    "    y = b0 + b1*x + noise\n",
    "    \n",
    "    return(cbind(y, rep(1, N), x))\n",
    "}\n",
    "\n",
    "set.seed(210)\n",
    "data = sim_data(N=25)\n",
    "data[1:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>4.41723362427488</li>\n",
       "\t<li>9.40293876474449</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4.41723362427488\n",
       "\\item 9.40293876474449\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4.41723362427488\n",
       "2. 9.40293876474449\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 4.417234 9.402939"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = data[,1]\n",
    "X = data[,2:3]\n",
    "b_hat = solve(t(X)%*%X)%*%t(X)%*%Y\n",
    "b_hat = c(b_hat)\n",
    "b_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our estimates are not too close to the true value of $\\beta$. But this is likely because we simulated a really small sample of data. Shortly, we explore what happens to the estimates when we increase the sample size. \n",
    "\n",
    "But for now we ask, what are we doing when obtaining this estimator? We can take a closer look graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(x=data[,3], y=data[,1], xlab=\"X_1\", ylab=\"Y\", bg=\"red\", pch=21,\n",
    "    col=\"red\")\n",
    "lines(x=seq(min(data[,3]), max(data[,3]), length.out=100), \n",
    "      y=b_hat[1] + b_hat[2]*seq(min(data[,3]), max(data[,3]), length.out=100),\n",
    "      col=\"blue\", lwd=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line is fit to minimize the average distance between itself and the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "We have covered the case where we've regressed $Y$ on a single regressor $X_1$ and a constant. This is univariate regression. But luckily, we've already explored regression by writing $X=(1,X_1)'$ as a vector. So now, just think that we have more regressors so that $X=(1,X_1,X_2,...,X_K)$ is a $K+1$ vector, i.e., that we have $K$ total regressors. Writing with $i$ subscripts this is $X_i = (1, X_{1,i},...,X_{K,i})'$.\n",
    "\n",
    "Because we have already solved for the linear regression parameters when $X_i$ is a vector, the math is the same, except the matrices and vectors are of a higher dimension. We still have $\\beta = E[XX']^{-1}E[XY]$ where $X$ is now a $(K+1)\\times 1$ vector. The OLS estimator of $\\beta$ is (in super matrix notation) $\\hat{\\beta}_N = (\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}'\\boldsymbol{Y}$ where $\\boldsymbol{X}$ is now an $N \\times (K+1)$ matrix. Note that $K=1$ is the univariate case from above.\n",
    "\n",
    "The main insight from multiple (or multivariate) linear regression is the interpretation of the parameters and how the parameters can change with the introduction of more explanatory variables. To help explain the intuition, let's consider the case when $K=2$, so that we have two explanatory variables, $X_{1,i}$ and $X_{2,i}$, and $X_i = (1, X_{1,i}, X_{2,i})'$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Regression Coefficients from Multiple Regression: The Frisch-Waugh Theorem\n",
    "Suppose that the linear regression of $Y$ on $X_1$ and $X_2$ can be denoted as $\\beta_0 + \\beta_1X_1 + \\beta_2X_2$ or $X'\\beta$ where $X\\equiv (1, X_1, X_2)$. Next consider the linear regression of $Y$ on $X_2$, denoted $BLP(Y|X_2)$ and the linear regression of $X_1$ on $X_2$ as $BLP(X_1|X_2)$. Let the residual from these regressions be defined as $U \\equiv Y - X'\\beta$, $\\tilde{Y} \\equiv Y - BLP(Y|X_2)$ and $\\tilde{X}_1\\equiv X_1 - BLP(X_1|X_2)$. By properties of BLP, $E[XU] = E[U] = E[\\tilde{X}_1] = E[\\tilde{Y}] = E[\\tilde{X}_1X_2] = E[\\tilde{Y}X_2]=0$. \n",
    "\n",
    "It turns out, we can interpret $\\beta_1$ as the slope coefficient of a regression of $\\tilde{Y}$ on $\\tilde{X}_1$. Why? Let's prove it! Let $\\tilde{\\beta}_1$ be the slope coefficient from a regression of $\\tilde{Y}$ on $\\tilde{X}_1$.\n",
    "$$\n",
    "\\tilde{\\beta}_1  = \\frac{Cov[\\tilde{Y},\\tilde{X}_1]}{Var[\\tilde{X}_1]} = \\frac{E[\\tilde{Y}\\tilde{X}_1]}{E[\\tilde{X}_1^2]}\n",
    "$$\n",
    "$$\n",
    "= \\frac{E[(Y - BLP(Y|X_2))\\tilde{X}_1]}{E[\\tilde{X}_1^2]} = \\frac{E[Y\\tilde{X}_1]}{E[\\tilde{X}_1^2]}\n",
    "$$\n",
    "$$\n",
    "= \\frac{E[(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + U)\\tilde{X}_1]}{E[\\tilde{X}_1^2]}\n",
    "$$\n",
    "$$\n",
    "= \\beta_1\\frac{E[X_1\\tilde{X}_1]}{E[\\tilde{X}_1^2]} = \\beta_1\\frac{E[(\\tilde{X}_1 + BLP(X_1|X_2))\\tilde{X}_1]}{E[\\tilde{X}_1^2]}\n",
    "$$\n",
    "$$\n",
    "= \\beta_1\\frac{E[\\tilde{X}_1\\tilde{X}_1]}{E[\\tilde{X}_1^2]} = \\beta_1.\n",
    "$$\n",
    "What we have just done is to show that $\\beta_1$ is the (linear) \"effect\" of $X_1$ on $Y$ after \"controlling\" (linearly) for $X_2$. This result is known as the **Frisch-Waugh theorem**.\n",
    "\n",
    "One can also prove that the OLS estimator corresponding to these quantities is also the same, i.e., \n",
    "that\n",
    "$$\n",
    "\\hat{\\tilde{\\beta}}_{1,N} = \\frac{\\sum_{i=1}^N(\\hat{\\tilde{Y}}_i - \\bar{\\tilde{Y}}_N)(\\hat{\\tilde{X}}_{1,i} - \\bar{\\tilde{X}}_{1,N})}{\\sum_{i=1}^N(\\hat{\\tilde{X}}_{1,i} - \\bar{\\tilde{X}}_{1,N})^2} = \\hat{\\beta}_{1,N}\n",
    "$$\n",
    "where $\\hat{\\tilde{Y}}_i$ is the estimated residual of a regression of $Y$ on $X_2$ and $\\hat{\\tilde{X}}_{1,N}$ is the estimated residual of a regression of $X_1$ on $X_2$. To see this, let's do a Monte Carlo exercise, calculating estimates of the regression by first using a single regression and second by separately regression $Y$ and $X_1$ on $X_2$, and then doing residual regression. Let \n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "X_1 \\\\ X_2 \n",
    "\\end{pmatrix} \\sim\n",
    "\\mathcal{N}\n",
    "\\left(\\begin{pmatrix}\n",
    "1 \\\\ 2\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "1 & 0.5 \\\\ 0.5 & 2\n",
    "\\end{pmatrix}\n",
    "\\right),\n",
    "$$\n",
    "$N=1000$ and $U = Y - BLP(Y|X_1,X_2)\\sim \\mathcal{N}(0,10)$ and $\\beta=(2, 3, 4)'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "3.16528331101283"
      ],
      "text/latex": [
       "3.16528331101283"
      ],
      "text/markdown": [
       "3.16528331101283"
      ],
      "text/plain": [
       "[1] 3.165283"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(MASS)\n",
    "sim_data2 = function(N) {\n",
    "    mean = c(1, 2)\n",
    "    cov = cbind(c(1, 0.5), c(0.5, 2))\n",
    "    x = mvrnorm(N, mu=mean, Sigma=cov)\n",
    "    x = cbind(rep(1, N), x)\n",
    "    noise = rnorm(N, mean=0, sd=sqrt(10))\n",
    "    \n",
    "    b = c(2, 3, 4)\n",
    "    dim(b) <- c(3, 1)\n",
    "    \n",
    "    y = x%*%b + noise\n",
    "    \n",
    "    cbind(y, x)\n",
    "}\n",
    "\n",
    "N = 1000\n",
    "set.seed(210)\n",
    "\n",
    "# simulating data\n",
    "data = sim_data2(N)\n",
    "\n",
    "reg = function(Y, X){solve(t(X)%*%X)%*%t(X)%*%Y}\n",
    "res = function(Y, X){Y - X%*%reg(Y, X)}\n",
    "\n",
    "# One regression procedure\n",
    "Y = data[, 1]\n",
    "X = data[, 2:4]\n",
    "b = reg(Y, X)\n",
    "b[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "3.16528331101285"
      ],
      "text/latex": [
       "3.16528331101285"
      ],
      "text/markdown": [
       "3.16528331101285"
      ],
      "text/plain": [
       "[1] 3.165283"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# two regression procedure\n",
    "X1 = data[, 3]\n",
    "X2 = data[, c(2, 4)]\n",
    "\n",
    "# Step 1: get residuals\n",
    "# residual of Y on X2\n",
    "Y.til = res(Y, X2)\n",
    "\n",
    "# residual of X1 on X2\n",
    "X1.til = res(X1, X2)\n",
    "\n",
    "# Step 2: regress residuals on residuals\n",
    "X.til = cbind(rep(1, N), X1.til)\n",
    "b.til = reg(Y.til, X.til)\n",
    "\n",
    "b.til[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain numerically identical estimates using both procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship between Multivariate and Univariate Regression Parameters\n",
    "Now that we've introduced multivariate regression, we can ask an additional question which will help us in the future. Consider the linear regression of $Y$ on $X_1$, denoted $BLP(Y|X_1) = \\beta_0 + \\beta_1X_1$. How would $\\beta_1$ change if we had included $X_2$ in the regression?\n",
    "\n",
    "To answer this question, write the linear regression of $Y$ on $X_1$ and $X_2$ as $BLP(Y|X_1,X_2)=\\gamma_0 + \\gamma_1X_1 + \\gamma_2X_2$ and define the residual of this regression as $U = Y - BLP(Y|X_1,X_2)$. Recall that $U$ has the property that $E[U] = E[X_1U] = E[X_2U] = 0$. \n",
    "\n",
    "Our question reduces to, what is the relationship between $\\beta_1$ and $\\gamma_1$?\n",
    "\\begin{align}\n",
    "\\beta_1 & = \\frac{Cov[Y,X_1]}{Var[X_1]} = \\frac{Cov[\\gamma_0 + \\gamma_1X_1 + \\gamma_1X_2 + U,X_1]}{Var[X_1]} \\\\\n",
    "& = \\gamma_1 + \\gamma_2\\frac{Cov[X_1,X_2]}{Var[X_1]},\n",
    "\\end{align}\n",
    "so that $\\beta_1=\\gamma_1$ if $\\gamma_2=0$ or $Cov[X_1,X_2]=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure of Fit: The R-Squared\n",
    "How much of the variance in $Y$ is explained by the variation in $X$? We can answer this question by looking at the $R^2$:\n",
    "$$\n",
    "R^2 = \\frac{\\beta'Var[X]\\beta}{Var[Y]} = 1 - \\frac{Var[U]}{Var[Y]},\n",
    "$$\n",
    "where $U$ is defined as $U = Y - X'\\beta$. We can estimate this with\n",
    "$$\n",
    "R^2_N = 1 - \\frac{\\sum_{i=1}^N\\hat{U}_i^2}{\\sum_{i=1}^N(Y_i - \\bar{Y}_N)^2} = 1 - \\frac{\\sum_{i=1}^N(Y_i - X_i'\\hat{\\beta}_N)^2}{\\sum_{i=1}^N(Y_i - \\bar{Y}_N)^2}\n",
    "$$\n",
    "or utilize an unbiased estimator, called the adjusted $R^2$, denoted $\\bar{R}^2_N$,\n",
    "$$\n",
    "\\bar{R}^2_N = 1 - \\frac{N - 1}{N - K - 1}\\frac{\\sum_{i=1}^N\\hat{U}_i^2}{\\sum_{i=1}^N(Y_i - \\bar{Y}_N)^2}\n",
    "$$\n",
    "where $K$ is the number of regressors. In our example above, $K=2$. The advantage of the estimator $R^2_N$ is that it is bound between $0$ (no variance explained by $X$) and $1$ (all of the variance explained by $X$). But this also means that it must be biased, since our estimator will have almost zero probability of being exactly $0$. Another drawback of using $R^2$ is that adding an additional explanatory variable always increases the $R^2$, even if this variable adds no new \"real\" information for explaining variance in $Y$. This is why the adjusted $R^2$, $\\bar{R}^2_N$ is often used, as it penalizes the addition of more variables to the regression.\n",
    "\n",
    "Let's calculate $R^2_N$ and $\\bar{R}^2_N$ for the data we simulated and compare it to the truth. Since $Var[U] = 10$ and\n",
    "$$\n",
    "Var[Y] = Var[\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + U] = \\beta_1^2Var[X_1] + \\beta_2^2Var[X_2] + 2\\beta_1\\beta_2Cov[X_1,X_2] + Var[U]\n",
    "$$\n",
    "$$\n",
    "= 9\\cdot 1 + 16\\cdot 2 + 2\\cdot12\\cdot 0.5 + 10 = 63\n",
    "$$\n",
    "then the true $R^2=1 - \\frac{Var[U]}{Var[Y]} = 1 - \\frac{10}{63} = 53/63$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.841269841269841"
      ],
      "text/latex": [
       "0.841269841269841"
      ],
      "text/markdown": [
       "0.841269841269841"
      ],
      "text/plain": [
       "[1] 0.8412698"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "53/63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.858854998810704</li>\n",
       "\t<li>0.858571859390064</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.858854998810704\n",
       "\\item 0.858571859390064\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.858854998810704\n",
       "2. 0.858571859390064\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.8588550 0.8585719"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "U = res(Y, X)\n",
    "R2 = 1 - sum(U^2)/sum((Y - mean(Y))^2)\n",
    "R2.adj = 1 - ((N - 1)/(N - 3))*sum(U^2)/sum((Y - mean(Y))^2)\n",
    "c(R2, R2.adj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
